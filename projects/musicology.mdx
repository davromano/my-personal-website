---
title: "\"Human-Like\" Music Generation"
slug: "musicology"
tags:
  - Data Science
desc: "With my team, I created ML models for music generation, including rhythm, melody, and musical form. Despite being new to this domain, I invested significant effort into metric design and model selection. The project also led me to explore techniques like GANs, Markov chains, and GMMs."

---
import Summary from '../src/components/Summary';


<Summary>
- Development of ML models for music generation, divided into three distinct phases: rhythm, melody, and musical form.
- Application of ML in a domain that was new to me. I dedicated considerable effort in the metric design and the choice of the model.
- Exploration of techniques such as GANs, Markov chains and GMMs.
</Summary>

## Introduction

For the course Digital Musicology at EPFL me and my team we developed three models for generating respectively human-like rhythm, melody and musical piece.
These works were based on a corpus of Swedish music.
In each of the three laboratories we defined: 

<div className="w-3/5 items-center text-left text-lg">

<p className="text-dark-minsk p-2" style={{ fontFamily: 'tt-norms' }}>1 - evaluation metric </p>
<p className="text-dark-minsk p-2" style={{ fontFamily: 'tt-norms' }}>2 - encoding for the feature we wanted to work with </p>
<p className="text-dark-minsk p-2" style={{ fontFamily: 'tt-norms' }}>3 - definition of the model </p>
<p className="text-dark-minsk p-2" style={{ fontFamily: 'tt-norms' }}>4 - train the model on our dataset</p>
<p className="text-dark-minsk p-2" style={{ fontFamily: 'tt-norms' }}>5 - Generation</p>
<p className="text-dark-minsk p-2" style={{ fontFamily: 'tt-norms' }}>5 - Analyze the results</p>

</div>

Our corpus consisted of 602 pieces of the Slangpolska genre; a traditional Swedish folk dance music that originated in the province of Dalarna. Rhythmically it is most notably characterized by a 3/4 or 3/8 time signature, and a distinct syncopated rhythm.

## Rhythm

In our laboratory, we compared randomly generated 3/4 time musical bars with those produced by our model, trained on provided data.

<img src={'/music-workflow.png'} alt="Ciao"  style={{ width: '600px' }} className="mb-4"/>

Firstly, we defined our distance metric. 
We have resorted to the use of the Hamming distance (as distance metric) on a vectorized version of the rhythmical data. 
More precisely, each vectorized output generated by our models (whether random or GAN model) is compared to each vectorized rhythm originating from the dataset. 
The minimum of those distances is taken as the evaluation metric.

We then use the same distance metric within the initial corpus. 
This auto-correlation within the dataset allowed us to establish a reference value for our results.

Finally, we calculated mean, median, and standard deviation of the evaluation metric for the original corpus, random music generation, and GAN-based music generation.

To encode the rhythm we extracted the most helpful rhythm information from the dataset and kept only the inter-onset-interval (IOI) of each note. 
We parsed .abc encoded music files using music21 to obtain IOIs, encoding them as 0s and 1s, with each value representing a 16th-note division.

<img src={'/music-ioi.png'} alt="Ciao"  style={{ width: '350px' }} className="mb-4"/>

The random model is simply a generation of a string of 1s and 0s while the model we use is a Generative Adversarial Network (GAN).  
It is a deep learning model that consists of two neural networks: a generator and a discriminator. 
Both the generator and discriminator consists of eight fully connected layers. 
The Generator outputs a fake sample that should resemble real data. 
The discriminator takes areal and generated rhythm and tries distiniguishing between them. We implemented two loss function, one for the generator and one for the discriminator. 
The generator loss measures how well the generator is able to fool the discriminator, while the discriminator loss measures how well the discriminator is able to distinguish between real and fake samples.

<img src={'/music-gan.png'} alt="Ciao"  style={{ width: '350px' }} className="mb-4"/>

The results are showed in this table.

<img src={'/music-results1.png'} alt="Ciao"  style={{ width: '350px' }} className="mb-4"/>

It's promising that the average min distance for the GAN model is much closer to the real dataset distance than the random model. However, the SD of our model is far lower compared to the real dataset, this could mean that our results are all quite similar to each other.

We analyzed also the histogram distribution of note durations. 
We can observe that in general our model has the same distribution of notes duration, even if it lacks 1/16th notes. Furthermore, to analyze the results we computed co-occurrence matrices for each dataset.

<img src={'/music-distribution-note.png'} alt="Ciao"  style={{ width: '350px' }} className="mb-4"/>

We can observe from these matrices and our distribution of notes duration that in our generated results there is a tendency towards producing more 1/8th notes, compared to the original dataset where there are more 1/16th notes.

## Melody

The melodiesin the dataset primarily adhere to diatonic scales, employing a combination of major and minor modes within a 3/4 time signature.

During preprocessing, we normalized all scores to either C major for major keys or A minor for minor keys. 
We also categorized our dataset into major and minor keys for separate training.

To represent pitches, we extended our 16th note division representation to include MIDI pitch values, where zero signifies the absence of an onset.

For creating melodies, we wanted to consider pitches, the position of notes within the song and how they relate to neighbouring notes. 
We introduced a multi-layered structure aligning with the 3/4 metrical grid, creating a tree-like model. 
Each layer corresponds to the "importance" of beats within the metrical grid, totaling six layers in our 8-bar sample dataset.

<img src={'/music-metrical.png'} alt="Ciao"  style={{ width: '500px' }} className="mb-4"/>

Our model adopts a hierarchical Markov chain approach, incorporating multiple Markov chains, each trained on transition matrices for individual layers of the metrical grid. Melody generation begins at the first important beat, progressing through the hierarchy following the metrical grid's structure. 
Pitch generation complements a pre-generated rhythm pattern from our earlier model.

In terms of evaluation, we employed various metrics, including a novel distance metric, mean absolute interval (MAI), and pitch distribution (measured using KL divergence). 
The distance metric calculated the Euclidean distance between generated samples and the nearest score in the original dataset. 
It considered pitch distances weighted by the layer of the metrical grid.

In general the distance metric shows us that the generated melodies are much closer than randomly generated ones to the original dataset, however the subdivision in major and minor didn't improve the result.

The MAI metric showed that our generated results align more closely with the original dataset's interval distribution but tend to produce larger intervals.

<img src={'/music-mai.png'} alt="Ciao"  style={{ width: '300px' }} className="mb-4"/>

Pitch distribution analysis via KL divergence indicated that our model statistically replicates musical pitch characteristics well, though it doesn't guarantee the quality of the generated music from a human listening perspective.
## Musical piece and musical form


In the final phase of our laboratory work, we accomplished the generation of complete 8-bar musical pieces, encompassing rhythm, melodies, and musical structure, including characteristics like score repetitions.

To prepare the data, we initially extracted pitch sets from MIDI values and divided them into bars. These bars were then transformed into Bag of Words (BoW) representations. Subsequently, we utilized a Gaussian Mixture Model (GMM) to cluster bars based on their harmonic content into seven clusters, each representing a diatonic scale note.

Our model consisted of four key steps: template generation, guide tone generation, regularization, and elaboration. 

In the first step, we employed a layered Markov chain approach to create harmonic patterns with constraints introduced by repetition markers.

<img src={'/music-repetition.png'} alt="Ciao"  style={{ width: '350px' }} className="mb-4"/>

In the second step, we derived probability distributions from each cluster, facilitating the creation of guide tones by sampling notes.
Regularization came next, where we adjusted notes within a range relative to their neighboring notes, ensuring a harmonious transition.
In the final step, we added less critical notes or "decorations" using a Markov chain, considering relationships between guide tones to create intricate and complex melodies.

For the evaluation we used the same metrics as in the lab 2 and our results were quite satisfying. This time we also implemented self-similarity matrices to qualitatively analyze the repetition patterns of our musical pieces.

<img src={'/music-similarity.png'} alt="Ciao"  style={{ width: '350px' }} className="mb-4"/>

These matrices demonstrated that we successfully imposed the repetition pattern on our pieces.

Here is an example of our music generation.

<video controls="controls" style={{ width: '50%' }}>
<source src="/music-video.mp4" type="video/mp4"/>
</video>

## My experience

**Domain knowledge is important**: delving into music theory, a new realm for me, presented a challenge in comprehending the domain and selecting the most suitable ML techniques.

**ML application process**: I noticed how important is to methodically track each step in the process. It's not solely about the model or technique; equally crucial are decisions regarding data encoding and the choice of evaluation metrics.

<p className="text-dark-minsk justify-center pt-4"  style={{ fontFamily: 'tt-norms' }}>Team: Davide Romano, Barbara Ruvolo, Jeremi Do Dinh, Mengjie Zou</p>

<p className="text-dark-minsk justify-center "  style={{ fontFamily: 'tt-norms' }}>Author: Davide Romano</p>